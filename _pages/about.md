---
permalink: /
title: "About Me"
excerpt: "Ph.D. Student | Northeastern University"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a second-year Ph.D. student in the College of Engineering at Northeastern University, advised by [Prof. Yun Raymond Fu](https://www1.ece.neu.edu/~yunfu/) in the [SMILE Lab](https://fulab.sites.northeastern.edu/).  
Before joining Northeastern, I received my B.S. and M.S. degrees from Xidian University, China, where I was advised by [Prof. Xuefeng Liang](https://web.xidian.edu.cn/xliang/en/index.html).  
During my masterâ€™s studies, I also spent six months at Kyoto University, working with [Prof. Takatsune Kumada](https://kdb.iimc.kyoto-u.ac.jp/profile/en.a61c204316cdb5fc.html#display-items_basic-information).

My research interests include **multimodal large language models (MLLMs)** and **vision language models (VLMs)**, with a focus on **hallucination detection & mitigation** [SHIELD], **video understanding** [D-CoDe], and **Layout Understanding** [MASON].

In Summer 2025, I interned at **Adobe Research**.

Full CV: [Curriculum Vitae](../files/cv-yiyang.pdf)


---
# News

<div class="news-container">
  <div class="news-item">
    <span class="news-date">[Jan. 2026]</span>
    <span class="news-content">One paper <em>SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense</em> accepted by <strong>ICLR 2026</strong>.</span>
  </div>
  <div class="news-item">
    <span class="news-date">[Aug. 2025]</span>
    <span class="news-content">One paper <em>D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition</em> accepted by <strong>EMNLP 2025</strong>.</span>
  </div>
</div>  


---
# Publications ([Google Scholar](https://scholar.google.com/citations?user=A0H2ZYQAAAAJ))

<div class="publications-list">
  <div class="publication-item">
    <div class="publication-title">**MASON: Compositional Design Layout Understanding in VLMs through Multimodal Alignment and Structural Perception**</div>
    <div class="publication-authors">**Yiyang Huang**, Zhaowen Wang, Simon Jenni, Jing Shi, Yun Fu</div>
    <div class="publication-venue"><span class="venue-tag under-review">CVPR under-review</span></div>
  </div>

  <div class="publication-item">
    <div class="publication-title">**SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense**</div>
    <div class="publication-authors">**Yiyang Huang**, Liang Shi, Yitian Zhang, Yi Xu, Yun Fu</div>
    <div class="publication-venue"><span class="venue-tag accepted">International Conference on Learning Representations (ICLR)</span>, 2026</div>
    <div class="publication-links">
      <a href="https://arxiv.org/abs/2510.16596" class="pub-link">Paper</a>
      <a href="https://arxiv.org/abs/2510.16596" class="pub-link">Code</a>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-title">**D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition**</div>
    <div class="publication-authors">**Yiyang Huang**, Yizhou Wang, Yun Fu</div>
    <div class="publication-venue"><span class="venue-tag accepted">Empirical Methods in Natural Language Processing (EMNLP)</span>, 2025</div>
    <div class="publication-links">
      <a href="https://arxiv.org/abs/2510.08818" class="pub-link">Paper</a>
      <a href="https://github.com/hukcc/D-CoDe" class="pub-link">Code</a>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-title">**LipReading for Low-resource Languages by Language Dynamic LoRA**</div>
    <div class="publication-authors">Shuai Zou, Xuefeng Liang, **Yiyang Huang**</div>
    <div class="publication-venue"><span class="venue-tag accepted">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, 2025</div>
    <div class="publication-links">
      <a href="https://ieeexplore.ieee.org/abstract/document/10889645" class="pub-link">Paper</a>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-title">**CALLip: Lipreading using Contrastive and Attribute Learning**</div>
    <div class="publication-authors">**Yiyang Huang**, Xuefeng Liang, Chaowei Fang</div>
    <div class="publication-venue"><span class="venue-tag accepted">ACM International Conference on Multimedia (ACMMM)</span>, 2021</div>
    <div class="publication-links">
      <a href="https://dl.acm.org/doi/10.1145/3474085.3475420" class="pub-link">Paper</a>
    </div>
  </div>
</div>

---
# Academic Service
- **Journal Reviewer**:  
  [ACM Transactions on Knowledge Discovery from Data (TKDD)](https://dl.acm.org/journal/tkdd)  

---
# Honors & Awards

<div class="awards-list">
  <div class="award-item">
    <span class="award-year">2022</span>
    <span class="award-text">Outstanding Student, Xidian University</span>
  </div>
  <div class="award-item">
    <span class="award-year">2021</span>
    <span class="award-text">National Scholarship, China</span>
  </div>
  <div class="award-item">
    <span class="award-year">2021</span>
    <span class="award-text">Undergraduate Computer Design Competition (1st Prize), China</span>
  </div>
  <div class="award-item">
    <span class="award-year">2019</span>
    <span class="award-text">RoboMaster National Robotics Competition (2nd Prize), China</span>
  </div>
  <div class="award-item">
    <span class="award-year">2019</span>
    <span class="award-text">ICRA AI Challenge (3rd Prize)</span>
  </div>
</div>  

---
# Teaching Experience

- **Teaching Assistant (TA)**: DS 5110 *Essentials of Data Science*, Fall 2025  

---
# Contact

<div class="contact-info">
  <div class="contact-item">
    <strong>Email:</strong> yiyang.huang.hukcc (at) gmail (dot) com / huang.yiyan (at) northeastern (dot) edu
  </div>
  <div class="contact-item">
    <strong>WeChat:</strong> hukcc369
  </div>
</div>

<style>
/* News Styles */
.news-container {
  margin: 1.5em 0;
}

.news-item {
  margin-bottom: 1.2em;
  line-height: 1.8;
  padding-left: 1.5em;
  position: relative;
  border-left: 3px solid var(--global-border-color, #e0e0e0);
  padding-left: 1.2em;
  transition: all 0.3s ease;
}

.news-item:hover {
  border-left-color: var(--global-theme-color, #0066cc);
  padding-left: 1.5em;
}

.news-date {
  font-weight: 600;
  color: var(--global-theme-color, #0066cc);
  margin-right: 0.5em;
}

.news-content {
  color: var(--global-text-color);
}

/* Publications Styles */
.publications-list {
  margin: 1.5em 0;
}

.publication-item {
  margin-bottom: 2em;
  padding: 1.2em;
  border-radius: 6px;
  background-color: var(--global-bg-color);
  border: 1px solid var(--global-border-color, #e0e0e0);
  transition: all 0.3s ease;
}

.publication-item:hover {
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  border-color: var(--global-theme-color, #0066cc);
  transform: translateY(-2px);
}

.publication-title {
  font-size: 1.05em;
  margin-bottom: 0.5em;
  line-height: 1.5;
}

.publication-authors {
  margin-bottom: 0.5em;
  color: var(--global-text-color);
  font-size: 0.95em;
}

.publication-venue {
  margin-bottom: 0.8em;
  font-size: 0.9em;
}

.venue-tag {
  display: inline-block;
  padding: 0.2em 0.6em;
  border-radius: 4px;
  font-size: 0.9em;
  font-weight: 500;
  margin-right: 0.5em;
}

.venue-tag.accepted {
  background-color: #e8f5e9;
  color: #2e7d32;
}

.venue-tag.under-review {
  background-color: #fff3e0;
  color: #e65100;
}

.publication-links {
  margin-top: 0.5em;
}

.pub-link {
  display: inline-block;
  margin-right: 1em;
  padding: 0.3em 0.8em;
  background-color: var(--global-theme-color, #0066cc);
  color: white;
  text-decoration: none;
  border-radius: 4px;
  font-size: 0.85em;
  transition: all 0.2s ease;
}

.pub-link:hover {
  background-color: var(--global-theme-color, #0052a3);
  transform: translateY(-1px);
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
}

/* Awards Styles */
.awards-list {
  margin: 1.5em 0;
}

.award-item {
  display: flex;
  align-items: baseline;
  margin-bottom: 0.8em;
  padding: 0.5em 0;
  border-bottom: 1px dotted var(--global-border-color, #e0e0e0);
}

.award-item:last-child {
  border-bottom: none;
}

.award-year {
  min-width: 60px;
  font-weight: 600;
  color: var(--global-theme-color, #0066cc);
  margin-right: 1em;
}

.award-text {
  flex: 1;
  color: var(--global-text-color);
}

/* Contact Styles */
.contact-info {
  margin: 1.5em 0;
}

.contact-item {
  margin-bottom: 0.8em;
  line-height: 1.8;
}

.contact-item strong {
  color: var(--global-theme-color, #0066cc);
  margin-right: 0.5em;
}

/* Responsive adjustments */
@media (max-width: 768px) {
  .news-item {
    padding-left: 1em;
  }
  
  .publication-item {
    padding: 1em;
  }
  
  .award-item {
    flex-direction: column;
  }
  
  .award-year {
    margin-bottom: 0.3em;
  }
}
</style>  
