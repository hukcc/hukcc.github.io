---
permalink: /
title: "About Me"
excerpt: "Ph.D. Student | Northeastern University"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<div class="intro-block">
  <p>
    I am a second-year Ph.D. student in the College of Engineering at
    <span class="kw">Northeastern University</span>, advised by
    <a class="kw" href="https://www1.ece.neu.edu/~yunfu/">Prof. Yun Raymond Fu</a> in the
    <a href="https://fulab.sites.northeastern.edu/">SMILE Lab</a>.
  </p>
  <p>
    I received my B.S. and M.S. degrees from <span class="kw">Xidian University</span>,
    advised by <a class="kw" href="https://web.xidian.edu.cn/xliang/en/index.html">Prof. Xuefeng Liang</a>.
    During my master's studies, I visited <span class="kw">Kyoto University</span>,
    working with <a class="kw" href="https://kdb.iimc.kyoto-u.ac.jp/profile/en.a61c204316cdb5fc.html#display-items_basic-information">Prof. Takatsune Kumada</a>.
  </p>
  <div class="intro-focus">
    <div class="focus-title">Research interests: Multimodal LLMs, efficiency, and reliability, with a focus on</div>
    <div class="focus-item">
      <span class="focus-category">hallucination detection &amp; mitigation:</span>
      <span class="focus-tags">
        <a class="paper-tag" href="#pub-shield">SHIELD</a>
        <a class="paper-tag" href="#pub-videollm-hallucination">VideoLLM Hallucination Survey</a>
      </span>
    </div>
    <div class="focus-item">
      <span class="focus-category">video understanding:</span>
      <span class="focus-tags">
        <a class="paper-tag" href="#pub-dcode">D-CoDe</a>
      </span>
    </div>
    <div class="focus-item">
      <span class="focus-category">layout understanding:</span>
      <span class="focus-tags">
        <a class="paper-tag" href="#pub-mason">MASON</a>
      </span>
    </div>
  </div>
  <p class="intro-note">
    Actively seeking internship opportunities.
  </p>
</div>


---
# News

<div class="news-container">
  <div class="news-item">
    <span class="news-date">Jan. 2026</span>
    <span class="news-text">One paper <em>SHIELD</em> accepted by <span class="news-venue">ICLR 2026</span></span>
  </div>
  <div class="news-item">
    <span class="news-date">Dec. 2025</span>
    <span class="news-text">Passed the Ph.D. Qualifying Exam, thanks to my advisor and committee members.</span>
  </div>
  <div class="news-item">
    <span class="news-date">Aug. 2025</span>
    <span class="news-text">One paper <em>D-CoDe</em> accepted by <span class="news-venue">EMNLP 2025</span></span>
  </div>
  <div class="news-item">
    <span class="news-date">May 2025</span>
    <span class="news-text">Started Research Internship at <span class="news-venue">Adobe Research</span>.</span>
  </div>
  <div class="news-item">
    <span class="news-date">Sep. 2024</span>
    <span class="news-text">Started my journey at <span class="news-venue">Northeastern University</span>.</span>
  </div>
</div>

---
# Experience

<div class="exp-list">
  <div class="exp-item">
    <div class="exp-text">
      <div class="exp-org">SMILE Lab, Northeastern University, Boston</div>
      <div class="exp-role">Ph.D. Student, Sep. 2024 ~ Now</div>
      <div class="exp-supervisor">Supervisor: Prof. Yun Raymond Fu</div>
    </div>
    <div class="exp-media">
      <img src="/images/neu.png" alt="Northeastern University">
    </div>
  </div>
  <div class="exp-item">
    <div class="exp-text">
      <div class="exp-org">Adobe Research, San Jose</div>
      <div class="exp-role">Research Intern, May 2025 â€“ Nov 2025</div>
      <div class="exp-supervisor">Mentor: Zhaowen Wang; Simon Jenni; Jing Shi; </div>
    </div>
    <div class="exp-media">
      <img src="/images/Adobe.png" alt="Adobe Research">
    </div>
  </div>
  <div class="exp-item">
    <div class="exp-text">
      <div class="exp-org">Kyoto University, Kyoto</div>
      <div class="exp-role">Research Student, Sep 2023 â€“ Mar 2024</div>
      <div class="exp-supervisor">Supervisor: Prof. Takatsune Kumada</div>
    </div>
    <div class="exp-media">
      <img src="/images/Kyoto University Emblem.svg" alt="Kyoto University">
    </div>
  </div>
  <div class="exp-item">
    <div class="exp-text">
      <div class="exp-org">Xidian University, Xi'an</div>
      <div class="exp-role">Master Student, Sep. 2021 ~ Jun. 2024</div>
      <div class="exp-role">Undergraduate Student, Sep. 2017 ~ Jun. 2021</div>
      <div class="exp-supervisor">Supervisor: Prof. Xuefeng Liang</div>
    </div>
    <div class="exp-media">
      <img src="/images/xdu.png" alt="Xidian University">
    </div>
  </div>
</div>

---
# Publications ([Google Scholar](https://scholar.google.com/citations?user=A0H2ZYQAAAAJ))

<div class="pub-item" id="pub-videollm-hallucination">
  <div class="pub-badge">Submitted to ARR</div>
  <div class="pub-thumb">
    <span class="pub-thumb-link">
      <img class="pub-thumb-img" src="/images/pub_arr_hallucination.png" alt="ARR Survey" onerror="this.style.display='none'">
      <img class="pub-thumb-preview" src="/images/pub_arr_hallucination.png" alt="" aria-hidden="true">
    </span>
  </div>
  <div class="pub-content">
    <div class="pub-title">Distorted or Fabricated? A Survey on Hallucination in Video LLMs</div>
    <div class="pub-authors"><strong>Yiyang Huang</strong>, Yitian Zhang, Yizhou Wang, Mingyuan Zhang, Liang Shi, Huimin Zeng, Yun Fu</div>
    <div class="pub-tldr"><span class="tldr-label">TL;DR:</span> Provides a systematic taxonomy and analysis of hallucinations in video large language models, covering their types, causes, evaluation, and mitigation strategies.</div>
  </div>
</div>

<div class="pub-item" id="pub-mason">
  <div class="pub-badge">Submitted to CVPR</div>
  <div class="pub-thumb">
    <span class="pub-thumb-link">
      <img class="pub-thumb-img" src="/images/pub_mason.png" alt="MASON" onerror="this.style.display='none'">
      <img class="pub-thumb-preview" src="/images/pub_mason.png" alt="" aria-hidden="true">
    </span>
  </div>
  <div class="pub-content">
    <div class="pub-title">MASON: Compositional Design Layout Understanding in VLMs through Multimodal Alignment and Structural Perception</div>
    <div class="pub-authors"><strong>Yiyang Huang</strong>, Zhaowen Wang, Simon Jenni, Jing Shi, Yun Fu</div>
    <div class="pub-tldr"><span class="tldr-label">TL;DR:</span> Introduces a compositional layout understanding framework that integrates multimodal alignment and structural perception to reason about interacting elements in layered design layouts.</div>
  </div>
</div>

<div class="pub-item" id="pub-shield">
  <div class="pub-badge">ICLR 2026</div>
  <div class="pub-thumb">
    <span class="pub-thumb-link">
      <img class="pub-thumb-img" src="/images/pub_shield.png" alt="SHIELD" onerror="this.style.display='none'">
      <img class="pub-thumb-preview" src="/images/pub_shield.png" alt="" aria-hidden="true">
    </span>
  </div>
  <div class="pub-content">
    <div class="pub-title">SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense</div>
    <div class="pub-authors"><strong>Yiyang Huang</strong>, Liang Shi, Yitian Zhang, Yi Xu, Yun Fu</div>
    <div class="pub-tldr"><span class="tldr-label">TL;DR:</span> Introduces a training-free framework that mitigates object hallucinations in LVLMs by addressing bias and vulnerability at the visual encoder level.</div>
    <div class="pub-links">
      <a href="https://arxiv.org/abs/2510.16596" class="pub-link">ðŸ“„ Paper</a>
      <a href="https://github.com/hukcc/SHIELD" class="pub-link">ðŸ’» Code</a>
    </div>
  </div>
</div>

<div class="pub-item" id="pub-dcode">
  <div class="pub-badge">EMNLP 2025</div>
  <div class="pub-thumb">
    <span class="pub-thumb-link">
      <img class="pub-thumb-img" src="/images/pub_dcode.png" alt="D-CoDe" onerror="this.style.display='none'">
      <img class="pub-thumb-preview" src="/images/pub_dcode.png" alt="" aria-hidden="true">
    </span>
  </div>
  <div class="pub-content">
    <div class="pub-title">D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition</div>
    <div class="pub-authors"><strong>Yiyang Huang</strong>, Yizhou Wang, Yun Fu</div>
    <div class="pub-tldr"><span class="tldr-label">TL;DR:</span> Proposes a training-free framework that combines dynamic visual compression and question decomposition to scale image-pretrained VLMs to long video understanding.</div>
    <div class="pub-links">
      <a href="https://arxiv.org/abs/2510.08818" class="pub-link">ðŸ“„ Paper</a>
      <a href="https://github.com/hukcc/D-CoDe" class="pub-link">ðŸ’» Code</a>
    </div>
  </div>
</div>

<div class="pub-item">
  <div class="pub-badge">ICASSP 2025</div>
  <div class="pub-thumb">
    <span class="pub-thumb-link">
      <img class="pub-thumb-img" src="/images/pub_lipreading.png" alt="LipReading" onerror="this.style.display='none'">
      <img class="pub-thumb-preview" src="/images/pub_lipreading.png" alt="" aria-hidden="true">
    </span>
  </div>
  <div class="pub-content">
    <div class="pub-title">LipReading for Low-resource Languages by Language Dynamic LoRA</div>
    <div class="pub-authors">Shuai Zou, Xuefeng Liang, <strong>Yiyang Huang</strong></div>
    <div class="pub-tldr"><span class="tldr-label">TL;DR:</span> Introduces dynamic low-rank fine-tuning and multilingual instruction tuning to improve lipreading performance in low-resource languages.</div>
    <div class="pub-links">
      <a href="https://ieeexplore.ieee.org/abstract/document/10889645" class="pub-link">ðŸ“„ Paper</a>
    </div>
  </div>
</div>

<div class="pub-item">
  <div class="pub-badge">ACMMM 2021</div>
  <div class="pub-thumb">
    <span class="pub-thumb-link">
      <img class="pub-thumb-img" src="/images/pub_callip.png" alt="CALLip" onerror="this.style.display='none'">
      <img class="pub-thumb-preview" src="/images/pub_callip.png" alt="" aria-hidden="true">
    </span>
  </div>
  <div class="pub-content">
    <div class="pub-title">CALLip: Lipreading using Contrastive and Attribute Learning</div>
    <div class="pub-authors"><strong>Yiyang Huang</strong>, Xuefeng Liang, Chaowei Fang</div>
    <div class="pub-tldr"><span class="tldr-label">TL;DR:</span> Combines attribute learning and audio-visual contrastive learning to improve robustness and discriminability in lipreading.</div>
    <div class="pub-links">
      <a href="https://dl.acm.org/doi/10.1145/3474085.3475420" class="pub-link">ðŸ“„ Paper</a>
    </div>
  </div>
</div>

<style>
/* Intro Styles */
.intro-block {
  margin: 0.5em 0 1.2em;
  padding: 0.6em 0 0.2em;
}

.intro-block p {
  margin: 0 0 0.7em;
  line-height: 1.7;
}

.intro-focus {
  margin-top: 0.2em;
}

.intro-note {
  display: inline-block;
  padding: 0.04em 0.22em;
  border-radius: 6px;
  background: #eef4ff;
  color: #1f6feb;
  border: none;
  margin-top: 0.2em;
  margin-bottom: 0;
  font-weight: 400;
  font-size: 1em;
}

.intro-meta {
  color: var(--global-text-color-light, #666);
}

.kw {
  font-weight: 600;
  color: var(--global-text-color, #222);
}

.exp-list {
  margin: 0.6em 0 1.2em;
}

.exp-item {
  display: flex;
  align-items: center;
  justify-content: space-between;
  gap: 1em;
  padding: 0.55em 0.75em;
  margin-bottom: 0.5em;
  background: #f6f8fc;
  border-left: 3px solid var(--global-link-color, #007bff);
  border-radius: 6px;
}

.exp-text {
  flex: 1;
  min-width: 0;
}

.exp-media {
  flex-shrink: 0;
  width: 72px;
  height: 72px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.exp-media img {
  max-width: 100%;
  max-height: 100%;
  width: auto;
  height: auto;
  object-fit: contain;
}

.exp-org {
  font-weight: 600;
  color: var(--global-text-color, #222);
  font-size: 0.95em;
}

.exp-role {
  margin-top: 0.15em;
  color: var(--global-text-color, #333);
  font-size: 0.9em;
}

.exp-supervisor {
  margin-top: 0.1em;
  color: var(--global-text-color-light, #666);
  font-size: 0.85em;
}

.focus-title {
  margin-bottom: 0.5em;
  font-weight: 600;
}

.focus-item {
  margin: 0.3em 0;
  display: flex;
  align-items: center;
  gap: 0.2em;
  flex-wrap: wrap;
}

.paper-tag {
  display: inline-block;
  padding: 0.12em 0.45em;
  margin-left: 0.35em;
  border-radius: 999px;
  background: #eef4ff;
  font-weight: 600;
  font-size: 0.78em;
  letter-spacing: 0.2px;
  text-decoration: none;
  border: 1px solid #c9dafd;
}

.paper-tag:hover {
  background: #e1ecff;
  text-decoration: none;
}

.focus-category {
  color: var(--global-text-color, #222);
  font-weight: 500;
}

.focus-tags {
  display: inline-flex;
  flex-wrap: wrap;
  gap: 0.35em;
}


/* News Styles */
.news-container {
  margin: 1em 0;
  max-height: 160px; /* shows about 2-3 items */
  overflow-y: auto;
  padding-right: 6px;
}

.news-item {
  display: flex;
  align-items: center;
  padding: 0.45em 0.65em;
  margin-bottom: 0.4em;
  background: linear-gradient(135deg, #f6f8fc 0%, #eef2f8 100%);
  border-left: 4px solid var(--global-link-color, #007bff);
  border-radius: 0 8px 8px 0;
  line-height: 1.45;
  gap: 0.5em;
}

.news-date {
  font-weight: 700;
  color: #1f6feb;
  background: #eef4ff;
  padding: 0.06em 0.4em;
  border-radius: 6px;
  letter-spacing: 0.2px;
  font-size: 0.82em;
}

.news-text {
  flex: 1;
  font-size: 0.88em;
}

.news-venue {
  font-weight: 600;
  color: var(--global-link-color, #007bff);
}

/* Publication Styles */
.pub-item {
  display: flex;
  align-items: stretch;
  margin-bottom: 0.85em;
  padding: 0.7em;
  background: #f6f8fc;
  border: 1px solid #e4e9f2;
  border-radius: 8px;
  transition: box-shadow 0.3s ease;
  gap: 0.7em;
  position: relative;
}

.pub-item:hover {
  box-shadow: 0 6px 16px rgba(0,0,0,0.18);
}

.pub-thumb {
  flex-shrink: 0;
  width: 160px;
  height: auto;
  background: #f6f8fc;
  border: 1px solid #e4e9f2;
  border-radius: 4px;
  overflow: visible;
  display: flex;
  align-items: center;
  justify-content: center;
  align-self: stretch;
}

.pub-thumb-link {
  display: flex;
  width: 100%;
  height: 100%;
  position: relative;
}

.pub-thumb-img {
  width: 100%;
  height: 100%;
  object-fit: cover;
  object-position: center;
  cursor: zoom-in;
  border-radius: 4px;
}

.pub-thumb-preview {
  position: absolute;
  left: calc(100% + 10px);
  top: 0;
  width: auto;
  height: auto;
  max-width: 55vw;
  max-height: 50vh;
  padding: 0;
  background: transparent;
  border: none;
  border-radius: 0;
  box-shadow: none;
  object-fit: contain;
  object-position: center;
  display: none;
  z-index: 10;
}

.pub-thumb-link:hover .pub-thumb-preview {
  display: block;
}

.pub-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  gap: 0.2em;
}

.pub-title {
  font-weight: bold;
  font-size: 0.97em;
  line-height: 1.3;
  color: var(--global-text-color, #333);
}

.pub-authors {
  font-size: 0.84em;
  color: var(--global-text-color-light, #666);
}

.pub-badge {
  position: absolute;
  top: 4px;
  left: 4px;
  font-size: 0.82em;
  padding: 0.2em 0.6em;
  border-radius: 8px;
  background: #eef4ff;
  color: #1f5bd6;
  border: 1px solid #c9dafd;
  z-index: 2;
  pointer-events: none;
  font-weight: 600;
}

.pub-tldr {
  margin-bottom: 0.3em;
  padding: 0.25em 0.5em;
  background: #f2f6ff;
  border-left: 3px solid #c9dafd;
  border-radius: 4px;
  color: var(--global-text-color-light, #666);
  font-size: 0.82em;
  line-height: 1.4;
}

.tldr-label {
  font-weight: 600;
  color: var(--global-text-color, #333);
}

.pub-links {
  display: flex;
  gap: 0.45em;
  flex-wrap: wrap;
  margin-top: 0;
}

.pub-link {
  font-size: 0.68em;
  padding: 0.18em 0.45em;
  background: #1f6feb;
  color: #fff !important;
  border: 1px solid #1f6feb;
  border-radius: 999px;
  text-decoration: none;
  transition: background 0.2s;
}

.pub-link:hover {
  background: #0b5ed7;
  text-decoration: none;
}

.intro-block a,
.news-container a,
.pub-item a,
.exp-list a {
  text-decoration: none;
}

/* Responsive tweaks */
@media (max-width: 900px) {
  .pub-item {
    flex-direction: column;
  }

  .pub-thumb {
    width: 100%;
    height: 180px;
  }

  .pub-thumb-preview {
    display: none !important;
  }

  .pub-meta-row {
    justify-content: flex-start;
  }

  .exp-item {
    flex-direction: column;
    align-items: flex-start;
  }

  .exp-media {
    width: 64px;
    height: 64px;
  }

  .pub-badge {
    font-size: 0.86em;
    padding: 0.22em 0.6em;
    font-weight: 600;
  }
}

@media (max-width: 600px) {
  .news-container {
    max-height: 140px;
  }

  .news-item {
    padding: 0.4em 0.55em;
  }

  .pub-thumb {
    height: 160px;
  }

  .pub-link {
    font-size: 0.66em;
    padding: 0.16em 0.4em;
  }
}

.visit-map {
  margin-top: 1.2em;
  padding: 1em;
  background: #f6f8fc;
  border: 1px solid #e4e9f2;
  border-radius: 8px;
  text-align: center;
}

.visit-map__widget {
  display: flex;
  justify-content: center;
  align-items: center;
}

.visit-map__widget a {
  display: inline-block;
}

.visit-map__widget img {
  max-width: 100%;
  height: auto;
  border-radius: 4px;
}
</style>

---
# Academic Service
- **Conference Reviewer**:  
  FG, ARR
- **Journal Reviewer**:  
  [ACM Transactions on Knowledge Discovery from Data (TKDD)](https://dl.acm.org/journal/tkdd)  

---
# Honors & Awards

- Outstanding Student, Xidian University, 2022  
- National Scholarship, China, 2021  
- Undergraduate Computer Design Competition (1st Prize), China, 2021  
- RoboMaster National Robotics Competition (2nd Prize), China, 2019  
- ICRA AI Challenge (3rd Prize), 2019  

---
# Teaching Experience

- **Teaching Assistant (TA)**: DS 5110 *Essentials of Data Science*, Fall 2025; DS 5020 *Fundamentals of Linear Algebra and Probability*, Spring 2026  

---
# Contact

Email: yiyang.huang.hukcc (at) gmail (dot) com / huang.yiyan (at) northeastern (dot) edu  
WeChat: hukcc369  
